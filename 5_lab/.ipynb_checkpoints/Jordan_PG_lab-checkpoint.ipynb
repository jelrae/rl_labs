{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Policy Gradient\n",
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the pg_autograde.py file into codegrade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports %%execwritefile command (executes cell and writes it into file). \n",
    "from custommagics import CustomMagics\n",
    "get_ipython().register_magics(CustomMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pg_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile pg_autograde.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc69f22067705372",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "import time\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6607b79e73a101a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-76a10fe31897025f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 3.1 Policy Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34f0712f792bbcca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to implement policy gradient, we will first implement a class with a policy network. Although in general this does not have to be the case, we will use an architecture very similar to the Q-network that we used (two layers with ReLU activation for the hidden layer). Since we have discrete actions, our model will output one value per action, where each value represents the (normalized!) probability of selecting that action. *Use the softmax activation function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a31440f9477f963",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to pg_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a pg_autograde.py\n",
    "\n",
    "class NNPolicy(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: input tensor (first dimension is a batch dimension)\n",
    "            \n",
    "        Return:\n",
    "            Probabilities of performing all actions in given input states x. Shape: batch_size x action_space_size\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return nn.functional.softmax(self.l2(nn.functional.relu(self.l1(x))), dim = 0)\n",
    "        \n",
    "    def get_probs(self, obs, actions):\n",
    "        \"\"\"\n",
    "        This function takes a tensor of states and a tensor of actions and returns a tensor that contains \n",
    "        a probability of perfoming corresponding action in all states (one for every state action pair). \n",
    "\n",
    "        Args:\n",
    "            obs: a tensor of states. Shape: batch_size x obs_dim\n",
    "            actions: a tensor of actions. Shape: batch_size x 1\n",
    "\n",
    "        Returns:\n",
    "            A torch tensor filled with probabilities. Shape: batch_size x 1.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        with  torch.no_grad():\n",
    "            probs = self.forward(obs)\n",
    "            print(probs.shape)\n",
    "            print(actions.shape)\n",
    "            print(probs)\n",
    "            print(actions)\n",
    "        action_probs = probs.gather(0, actions)\n",
    "            \n",
    "        return action_probs\n",
    "    \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: state as a tensor. Shape: 1 x obs_dim or obs_dim\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "#         obs = torch.tensor(obs)\n",
    "#         with torch.no_grad():\n",
    "        probs = self.forward(obs)\n",
    "            \n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "#         print(\"We sampling\", action)\n",
    "        return action\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9d280fe6520edc91",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]])\n",
      "tensor([[0.1026, 0.1066],\n",
      "        [0.0986, 0.0992],\n",
      "        [0.0977, 0.1021],\n",
      "        [0.0983, 0.0999],\n",
      "        [0.0983, 0.1027],\n",
      "        [0.0977, 0.0957],\n",
      "        [0.1034, 0.0995],\n",
      "        [0.1028, 0.0964],\n",
      "        [0.1030, 0.0980],\n",
      "        [0.0977, 0.0999]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 1])\n",
      "tensor([[0.1026, 0.1066],\n",
      "        [0.0986, 0.0992],\n",
      "        [0.0977, 0.1021],\n",
      "        [0.0983, 0.0999],\n",
      "        [0.0983, 0.1027],\n",
      "        [0.0977, 0.0957],\n",
      "        [0.1034, 0.0995],\n",
      "        [0.1028, 0.0964],\n",
      "        [0.1030, 0.0980],\n",
      "        [0.0977, 0.0999]])\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]])\n",
      "tensor([[0.1026],\n",
      "        [0.0986],\n",
      "        [0.0986],\n",
      "        [0.1026],\n",
      "        [0.1026],\n",
      "        [0.0986],\n",
      "        [0.1026],\n",
      "        [0.0986],\n",
      "        [0.1026],\n",
      "        [0.1026]])\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate and test if it works\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1234)\n",
    "policy = NNPolicy(num_hidden)\n",
    "\n",
    "states = torch.rand(10, 4)\n",
    "actions = torch.randint(low=0, high=2, size=(10,1))\n",
    "print(actions)\n",
    "\n",
    "# Does the outcome make sense?\n",
    "forward_probs = policy.forward(states)\n",
    "print(forward_probs)\n",
    "assert forward_probs.shape == (10,2), \"Output of forward has incorrect shape.\"\n",
    "sampled_action = policy.sample_action(states[0])\n",
    "assert sampled_action == 0 or sampled_action == 1, \"Output of sample action is not 0 or 1\"\n",
    "\n",
    "action_probs = policy.get_probs(states, actions)\n",
    "print(action_probs)\n",
    "assert action_probs.shape == (10,1), \"Output of get_probs has incorrect shape.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Monte Carlo REINFORCE\n",
    "\n",
    "Now we will implement the *Monte Carlo* policy gradient algorithm. Remember that this means that we will estimate returns for states by sample episodes. Compared to DQN, this means that we do *not* perform an update step at every environment step, but only at the end of each episode. This means that we should generate an episode of data, compute the REINFORCE loss (which requires computing the returns) and then perform a gradient step.\n",
    "\n",
    "* You can use `torch.multinomial` to sample from a categorical distribution.\n",
    "* The REINFORCE loss is defined as $- \\sum_t \\log \\pi_\\theta(a_t|s_t) G_t$, which means that you should compute the (discounted) return $G_t$ for all $t$. Make sure that you do this in **linear time**, otherwise your algorithm will be very slow! Note the - (minus) since you want to maximize return while you want to minimize the loss.\n",
    "\n",
    "To help you, we wrote down signatures of a few helper functions. Start by implementing a sampling routine that samples a single episode (similarly to the one in Monte Carlo lab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to pg_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a pg_autograde.py\n",
    "\n",
    "def sample_episode(env, policy):\n",
    "    \"\"\"\n",
    "    A sampling routine. Given environment and a policy samples one episode and returns states, actions, rewards\n",
    "    and dones from environment's step function as tensors.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        policy: A policy which allows us to sample actions with its sample_action method.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of tensors (states, actions, rewards, dones). All tensors should have same first dimension and \n",
    "        should have dim=2. This means that vectors of length N (states, rewards, actions) should be Nx1.\n",
    "        Hint: Do not include the state after termination in states.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    \n",
    "\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        \n",
    "        action = policy.sample_action(torch.tensor(state, dtype=torch.double).float())\n",
    "        s_next, r, done, _ = env.step(action)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(r)\n",
    "        dones.append(done)\n",
    "        state = s_next\n",
    "        \n",
    "        if dones[-1] == True:\n",
    "            break\n",
    "            \n",
    "    return states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's sample some episodes\n",
    "env = gym.envs.make(\"CartPole-v1\")\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1234)\n",
    "policy = NNPolicy(num_hidden)\n",
    "for episode in range(3):\n",
    "    trajectory_data = sample_episode(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement loss computation and training loop of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3f6e32c4931392bf",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to pg_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a pg_autograde.py\n",
    "\n",
    "def compute_reinforce_loss(policy, episode, discount_factor):\n",
    "    \"\"\"\n",
    "    Computes reinforce loss for given episode.\n",
    "\n",
    "    Args:\n",
    "        policy: A policy which allows us to get probabilities of actions in states with its get_probs method.\n",
    "\n",
    "    Returns:\n",
    "        loss: reinforce loss\n",
    "    \"\"\"\n",
    "    # Compute the reinforce loss\n",
    "    # Make sure that your function runs in LINEAR TIME\n",
    "    # Note that the rewards/returns should be maximized \n",
    "    # while the loss should be minimized so you need a - somewhere\n",
    "\n",
    "    loss = torch.tensor(0, dtype = torch.float, requires_grad = True)\n",
    "    print(loss)\n",
    "    G_t = torch.tensor([episode[2][i]**(len(episode[0])-1 - i) for i in np.arange(len(episode[0])-1, -1, -1)]).unsqueeze(0)\n",
    "    p = policy.get_probs(torch.tensor(episode[0]).float(),torch.tensor(episode[1]).unsqueeze(1)).float()\n",
    "    print(G_t)\n",
    "    print(p)\n",
    "    loss = torch.log(torch.dot(G_t,p).float())\n",
    "    \n",
    "    \n",
    "#   for i in np.arange(len(episode[0])-1, -1, -1):\n",
    "\n",
    "#         p = policy.get_probs(torch.tensor(episode[0][i]).float(),torch.tensor(episode[1][i]))\n",
    "#         G_t = episode[2][i]**(len(episode[0])-1 - i)\n",
    "#         loss -= np.log(p*G_t)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError\n",
    "\n",
    "def run_episodes_policy_gradient(policy, env, num_episodes, discount_factor, learn_rate, \n",
    "                                 sampling_function=sample_episode):\n",
    "    optimizer = optim.Adam(policy.parameters(), learn_rate)\n",
    "    \n",
    "    episode_durations = []\n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        episode = sample_episode(env, policy)\n",
    "        \n",
    "        loss = compute_reinforce_loss(policy, episode, discount_factor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                           \n",
    "        if i % 10 == 0:\n",
    "            print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                  .format(i, len(episode[0]), '\\033[92m' if len(episode[0]) >= 195 else '\\033[99m'))\n",
    "        episode_durations.append(len(episode[0]))\n",
    "        \n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing function for nicer plots\n",
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., requires_grad=True)\n",
      "torch.Size([24, 2])\n",
      "torch.Size([24, 1])\n",
      "tensor([[0.0414, 0.0309],\n",
      "        [0.0416, 0.0343],\n",
      "        [0.0414, 0.0307],\n",
      "        [0.0431, 0.0277],\n",
      "        [0.0413, 0.0305],\n",
      "        [0.0415, 0.0339],\n",
      "        [0.0414, 0.0304],\n",
      "        [0.0414, 0.0337],\n",
      "        [0.0414, 0.0302],\n",
      "        [0.0433, 0.0272],\n",
      "        [0.0414, 0.0300],\n",
      "        [0.0412, 0.0332],\n",
      "        [0.0417, 0.0369],\n",
      "        [0.0417, 0.0418],\n",
      "        [0.0418, 0.0367],\n",
      "        [0.0419, 0.0417],\n",
      "        [0.0416, 0.0465],\n",
      "        [0.0420, 0.0418],\n",
      "        [0.0417, 0.0469],\n",
      "        [0.0413, 0.0520],\n",
      "        [0.0411, 0.0582],\n",
      "        [0.0410, 0.0658],\n",
      "        [0.0414, 0.0746],\n",
      "        [0.0424, 0.0844]])\n",
      "tensor([[1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[0.0416],\n",
      "        [0.0414],\n",
      "        [0.0414],\n",
      "        [0.0416],\n",
      "        [0.0416],\n",
      "        [0.0414],\n",
      "        [0.0416],\n",
      "        [0.0414],\n",
      "        [0.0414],\n",
      "        [0.0416],\n",
      "        [0.0416],\n",
      "        [0.0416],\n",
      "        [0.0416],\n",
      "        [0.0414],\n",
      "        [0.0416],\n",
      "        [0.0416],\n",
      "        [0.0414],\n",
      "        [0.0416],\n",
      "        [0.0416],\n",
      "        [0.0416],\n",
      "        [0.0416],\n",
      "        [0.0416],\n",
      "        [0.0416],\n",
      "        [0.0416]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Double but got scalar type Float for argument #2 'tensor' in call to _th_dot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-00468f649b82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m episode_durations_policy_gradient = run_episodes_policy_gradient(\n\u001b[0;32m---> 12\u001b[0;31m     policy, env, num_episodes, discount_factor, learn_rate)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_durations_policy_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-02ddc7d1b475>\u001b[0m in \u001b[0;36mrun_episodes_policy_gradient\u001b[0;34m(policy, env, num_episodes, discount_factor, learn_rate, sampling_function)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mepisode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_reinforce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-02ddc7d1b475>\u001b[0m in \u001b[0;36mcompute_reinforce_loss\u001b[0;34m(policy, episode, discount_factor)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#     for i in np.arange(len(episode[0])-1, -1, -1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Double but got scalar type Float for argument #2 'tensor' in call to _th_dot"
     ]
    }
   ],
   "source": [
    "# Feel free to play around with the parameters!\n",
    "num_episodes = 500\n",
    "discount_factor = 0.99\n",
    "learn_rate = 0.001\n",
    "seed = 42\n",
    "env = gym.envs.make(\"CartPole-v1\")\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "policy = NNPolicy(num_hidden)\n",
    "\n",
    "episode_durations_policy_gradient = run_episodes_policy_gradient(\n",
    "    policy, env, num_episodes, discount_factor, learn_rate)\n",
    "\n",
    "plt.plot(smooth(episode_durations_policy_gradient, 10))\n",
    "plt.title('Episode durations per episode')\n",
    "plt.legend(['Policy gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the pg_autograde.py file into codegrade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
