{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Dynamic Programming\n",
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dp_autograde.py file into codegrade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports %%execwritefile command (executes cell and writes it into file). \n",
    "# All cells that start with %%execwritefile should be in dp_autograde.py file after running all cells.\n",
    "from custommagics import CustomMagics\n",
    "get_ipython().register_magics(CustomMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dp_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile dp_autograde.py\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Policy Evaluation (1 point)\n",
    "In this exercise we will evaluate a policy, e.g. find the value function of a policy. The problem we consider is the gridworld from Example 4.1 in the book. The environment is implemented as `GridworldEnv`, which is a subclass of the `Env` class from [OpenAI Gym](https://github.com/openai/gym). This means that we can interact with the environment. We can look at the documentation to see how we can interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridworld import GridworldEnv\n",
    "env = GridworldEnv()\n",
    "# Lets see what this is\n",
    "?env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have a quick look into the code\n",
    "??env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to evaluate a policy by using Dynamic Programming. For more information, see the [Intro to RL](https://drive.google.com/open?id=1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG) book, section 4.1. This algorithm requires knowledge of the problem dynamics in the form of the transition probabilities $p(s',r|s,a)$. In general these are not available, but for our gridworld we know the dynamics and these can be accessed as `env.P`. Note that we do not need to use a discount_factor for episodic tasks but make sure your implementation can handle this correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, True)],\n",
       "  1: [(1.0, 0, 0.0, True)],\n",
       "  2: [(1.0, 0, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, True)]},\n",
       " 1: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 2, -1.0, False)],\n",
       "  2: [(1.0, 5, -1.0, False)],\n",
       "  3: [(1.0, 0, -1.0, True)]},\n",
       " 2: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 6, -1.0, False)],\n",
       "  3: [(1.0, 1, -1.0, False)]},\n",
       " 3: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 7, -1.0, False)],\n",
       "  3: [(1.0, 2, -1.0, False)]},\n",
       " 4: {0: [(1.0, 0, -1.0, True)],\n",
       "  1: [(1.0, 5, -1.0, False)],\n",
       "  2: [(1.0, 8, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 5: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 6, -1.0, False)],\n",
       "  2: [(1.0, 9, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 6: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 10, -1.0, False)],\n",
       "  3: [(1.0, 5, -1.0, False)]},\n",
       " 7: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 11, -1.0, False)],\n",
       "  3: [(1.0, 6, -1.0, False)]},\n",
       " 8: {0: [(1.0, 4, -1.0, False)],\n",
       "  1: [(1.0, 9, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 9: {0: [(1.0, 5, -1.0, False)],\n",
       "  1: [(1.0, 10, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 10: {0: [(1.0, 6, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 9, -1.0, False)]},\n",
       " 11: {0: [(1.0, 7, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 15, -1.0, True)],\n",
       "  3: [(1.0, 10, -1.0, False)]},\n",
       " 12: {0: [(1.0, 8, -1.0, False)],\n",
       "  1: [(1.0, 13, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 13: {0: [(1.0, 9, -1.0, False)],\n",
       "  1: [(1.0, 14, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 14: {0: [(1.0, 10, -1.0, False)],\n",
       "  1: [(1.0, 15, -1.0, True)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 13, -1.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0.0, True)],\n",
       "  1: [(1.0, 15, 0.0, True)],\n",
       "  2: [(1.0, 15, 0.0, True)],\n",
       "  3: [(1.0, 15, 0.0, True)]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a moment to figure out what P represents. \n",
    "# Note that this is a deterministic environment. \n",
    "# What would a stochastic environment look like?\n",
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval_v(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "#     delta = 0\n",
    "#     V = np.zeros(env.nS)\n",
    "#     i = 0\n",
    "#     while delta < theta:\n",
    "#         print(i)\n",
    "#         for s in range(env.nS):\n",
    "#             if not (s == 0 or s == 15):\n",
    "#                 v = V[s]\n",
    "#                 sp = np.array([env.P[s][a][0][1] for a in range(0,4)])\n",
    "#                 rs = np.array([env.P[s][a][0][2] for a in range(0,4)])\n",
    "#                 p = np.array([env.P[s][a][0][0] for a in range(0,4)])\n",
    "#                 pi = np.array(policy[s][:])\n",
    "#                 V[s] = np.dot(pi.T,p*(rs + discount_factor * V[sp]))\n",
    "#                 delta = max(delta, np.abs(v-V[s]))\n",
    "#         i = i+1\n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    v = np.zeros(env.nS)\n",
    "    i = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        print(\"This is iter \", str(i))\n",
    "        v = np.copy(V)\n",
    "        for s in range(env.nS):\n",
    "            if not (s == 0 or s == 15):\n",
    "                sp = np.array([env.P[s][a][0][1] for a in range(0,4)])\n",
    "                rs = np.array([env.P[s][a][0][2] for a in range(0,4)])\n",
    "                p = np.array([env.P[s][a][0][0] for a in range(0,4)])\n",
    "                pi = np.array(policy[s][:])\n",
    "                V[s] = np.dot(pi.T,p*(rs + discount_factor * v[sp]))\n",
    "                delta = max(delta, np.abs(v[s]-V[s]))\n",
    "        print(\"The delta is \", delta)\n",
    "        if delta < theta:\n",
    "            break\n",
    "                \n",
    "        i = i+1\n",
    "        \n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is iter  0\n",
      "The delta is  1.0\n",
      "This is iter  1\n",
      "The delta is  1.0\n",
      "This is iter  2\n",
      "The delta is  1.0\n",
      "This is iter  3\n",
      "The delta is  0.96875\n",
      "This is iter  4\n",
      "The delta is  0.9375\n",
      "This is iter  5\n",
      "The delta is  0.89453125\n",
      "This is iter  6\n",
      "The delta is  0.8544921875\n",
      "This is iter  7\n",
      "The delta is  0.81103515625\n",
      "This is iter  8\n",
      "The delta is  0.770751953125\n",
      "This is iter  9\n",
      "The delta is  0.730255126953125\n",
      "This is iter  10\n",
      "The delta is  0.6925201416015625\n",
      "This is iter  11\n",
      "The delta is  0.6557655334472656\n",
      "This is iter  12\n",
      "The delta is  0.6213254928588867\n",
      "This is iter  13\n",
      "The delta is  0.5882596969604492\n",
      "This is iter  14\n",
      "The delta is  0.5571491718292236\n",
      "This is iter  15\n",
      "The delta is  0.527485579252243\n",
      "This is iter  16\n",
      "The delta is  0.49950262904167175\n",
      "This is iter  17\n",
      "The delta is  0.4729122780263424\n",
      "This is iter  18\n",
      "The delta is  0.44778872188180685\n",
      "This is iter  19\n",
      "The delta is  0.4239568174816668\n",
      "This is iter  20\n",
      "The delta is  0.40141887217760086\n",
      "This is iter  21\n",
      "The delta is  0.38005873272777535\n",
      "This is iter  22\n",
      "The delta is  0.3598478467174573\n",
      "This is iter  23\n",
      "The delta is  0.3407020762497268\n",
      "This is iter  24\n",
      "The delta is  0.32258116731372866\n",
      "This is iter  25\n",
      "The delta is  0.30541943855405407\n",
      "This is iter  26\n",
      "The delta is  0.28917376847448395\n",
      "This is iter  27\n",
      "The delta is  0.27379001551955184\n",
      "This is iter  28\n",
      "The delta is  0.2592261398323785\n",
      "This is iter  29\n",
      "The delta is  0.2454359065894245\n",
      "This is iter  30\n",
      "The delta is  0.23237999978143264\n",
      "This is iter  31\n",
      "The delta is  0.22001808767601005\n",
      "This is iter  32\n",
      "The delta is  0.20831414011977856\n",
      "This is iter  33\n",
      "The delta is  0.19723254226257225\n",
      "This is iter  34\n",
      "The delta is  0.1867406161957028\n",
      "This is iter  35\n",
      "The delta is  0.17680669684277106\n",
      "This is iter  36\n",
      "The delta is  0.16740130748806337\n",
      "This is iter  37\n",
      "The delta is  0.15849618885378192\n",
      "This is iter  38\n",
      "The delta is  0.15006482852898628\n",
      "This is iter  39\n",
      "The delta is  0.14208195501679555\n",
      "This is iter  40\n",
      "The delta is  0.1345237589079602\n",
      "This is iter  41\n",
      "The delta is  0.1273676154696588\n",
      "This is iter  42\n",
      "The delta is  0.1205921604290836\n",
      "This is iter  43\n",
      "The delta is  0.11417712643064704\n",
      "This is iter  44\n",
      "The delta is  0.108103351771895\n",
      "This is iter  45\n",
      "The delta is  0.10235267494911326\n",
      "This is iter  46\n",
      "The delta is  0.09690791387602715\n",
      "This is iter  47\n",
      "The delta is  0.09175279124077917\n",
      "This is iter  48\n",
      "The delta is  0.08687190204763695\n",
      "This is iter  49\n",
      "The delta is  0.08225065631786421\n",
      "This is iter  50\n",
      "The delta is  0.07787524330885987\n",
      "This is iter  51\n",
      "The delta is  0.07373258479577771\n",
      "This is iter  52\n",
      "The delta is  0.06981029976558517\n",
      "This is iter  53\n",
      "The delta is  0.06609666478283671\n",
      "This is iter  54\n",
      "The delta is  0.06258058077871098\n",
      "This is iter  55\n",
      "The delta is  0.05925153860689747\n",
      "This is iter  56\n",
      "The delta is  0.05609958851730923\n",
      "This is iter  57\n",
      "The delta is  0.053115309803800415\n",
      "This is iter  58\n",
      "The delta is  0.050289783074475736\n",
      "This is iter  59\n",
      "The delta is  0.04761456329634228\n",
      "This is iter  60\n",
      "The delta is  0.04508165476107351\n",
      "This is iter  61\n",
      "The delta is  0.042683487043778\n",
      "This is iter  62\n",
      "The delta is  0.04041289247617641\n",
      "This is iter  63\n",
      "The delta is  0.03826308465436057\n",
      "This is iter  64\n",
      "The delta is  0.03622763820372299\n",
      "This is iter  65\n",
      "The delta is  0.034300469541136636\n",
      "This is iter  66\n",
      "The delta is  0.032475818715564486\n",
      "This is iter  67\n",
      "The delta is  0.030748232176389223\n",
      "This is iter  68\n",
      "The delta is  0.029112546485048796\n",
      "This is iter  69\n",
      "The delta is  0.027563872874601714\n",
      "This is iter  70\n",
      "The delta is  0.026097582643533457\n",
      "This is iter  71\n",
      "The delta is  0.024709293317666692\n",
      "This is iter  72\n",
      "The delta is  0.023394855554336402\n",
      "This is iter  73\n",
      "The delta is  0.0221503407389676\n",
      "This is iter  74\n",
      "The delta is  0.02097202924440822\n",
      "This is iter  75\n",
      "The delta is  0.01985639931277916\n",
      "This is iter  76\n",
      "The delta is  0.018800116530190536\n",
      "This is iter  77\n",
      "The delta is  0.017800023860349512\n",
      "This is iter  78\n",
      "The delta is  0.016853132209082133\n",
      "This is iter  79\n",
      "The delta is  0.015956611490231865\n",
      "This is iter  80\n",
      "The delta is  0.01510778216723807\n",
      "This is iter  81\n",
      "The delta is  0.014304107244338837\n",
      "This is iter  82\n",
      "The delta is  0.01354318468407456\n",
      "This is iter  83\n",
      "The delta is  0.012822740227949936\n",
      "This is iter  84\n",
      "The delta is  0.012140620599144114\n",
      "This is iter  85\n",
      "The delta is  0.0114947870667379\n",
      "This is iter  86\n",
      "The delta is  0.010883309352323067\n",
      "This is iter  87\n",
      "The delta is  0.01030435986074707\n",
      "This is iter  88\n",
      "The delta is  0.009756208217776674\n",
      "This is iter  89\n",
      "The delta is  0.009237216098330947\n",
      "This is iter  90\n",
      "The delta is  0.008745832329790204\n",
      "This is iter  91\n",
      "The delta is  0.008280588255875188\n",
      "This is iter  92\n",
      "The delta is  0.007840093347070365\n",
      "This is iter  93\n",
      "The delta is  0.007423031044577755\n",
      "This is iter  94\n",
      "The delta is  0.007028154825395205\n",
      "This is iter  95\n",
      "The delta is  0.006654284476670824\n",
      "This is iter  96\n",
      "The delta is  0.006300302568238436\n",
      "This is iter  97\n",
      "The delta is  0.005965151112871325\n",
      "This is iter  98\n",
      "The delta is  0.005647828404111266\n",
      "This is iter  99\n",
      "The delta is  0.005347386022364731\n",
      "This is iter  100\n",
      "The delta is  0.005062926000263701\n",
      "This is iter  101\n",
      "The delta is  0.004793598138778776\n",
      "This is iter  102\n",
      "The delta is  0.00453859746614782\n",
      "This is iter  103\n",
      "The delta is  0.0042971618319676\n",
      "This is iter  104\n",
      "The delta is  0.004068569629236407\n",
      "This is iter  105\n",
      "The delta is  0.0038521376376436933\n",
      "This is iter  106\n",
      "The delta is  0.003647218981505773\n",
      "This is iter  107\n",
      "The delta is  0.003453201196414568\n",
      "This is iter  108\n",
      "The delta is  0.003269504398659251\n",
      "This is iter  109\n",
      "The delta is  0.003095579552088168\n",
      "This is iter  110\n",
      "The delta is  0.002930906827110391\n",
      "This is iter  111\n",
      "The delta is  0.002774994047044288\n",
      "This is iter  112\n",
      "The delta is  0.0026273752170880016\n",
      "This is iter  113\n",
      "The delta is  0.0024876091315277904\n",
      "This is iter  114\n",
      "The delta is  0.0023552780550772923\n",
      "This is iter  115\n",
      "The delta is  0.0022299864743331455\n",
      "This is iter  116\n",
      "The delta is  0.0021113599156521445\n",
      "This is iter  117\n",
      "The delta is  0.0019990438259256393\n",
      "This is iter  118\n",
      "The delta is  0.0018927025128938624\n",
      "This is iter  119\n",
      "The delta is  0.0017920181418027425\n",
      "This is iter  120\n",
      "The delta is  0.0016966897854615581\n",
      "This is iter  121\n",
      "The delta is  0.0016064325248414946\n",
      "This is iter  122\n",
      "The delta is  0.001520976597472412\n",
      "This is iter  123\n",
      "The delta is  0.0014400665912184252\n",
      "This is iter  124\n",
      "The delta is  0.0013634606808423655\n",
      "This is iter  125\n",
      "The delta is  0.0012909299052843437\n",
      "This is iter  126\n",
      "The delta is  0.001222257483309619\n",
      "This is iter  127\n",
      "The delta is  0.001157238165600205\n",
      "This is iter  128\n",
      "The delta is  0.0010956776212864838\n",
      "This is iter  129\n",
      "The delta is  0.0010373918571673357\n",
      "This is iter  130\n",
      "The delta is  0.000982206667735852\n",
      "This is iter  131\n",
      "The delta is  0.0009299571145433561\n",
      "This is iter  132\n",
      "The delta is  0.0008804870332248527\n",
      "This is iter  133\n",
      "The delta is  0.000833648566750611\n",
      "This is iter  134\n",
      "The delta is  0.0007893017235005573\n",
      "This is iter  135\n",
      "The delta is  0.0007473139588682898\n",
      "This is iter  136\n",
      "The delta is  0.0007075597790944244\n",
      "This is iter  137\n",
      "The delta is  0.0006699203661995057\n",
      "This is iter  138\n",
      "The delta is  0.0006342832228583006\n",
      "This is iter  139\n",
      "The delta is  0.0006005418361638704\n",
      "This is iter  140\n",
      "The delta is  0.0005685953592760029\n",
      "This is iter  141\n",
      "The delta is  0.000538348310005432\n",
      "This is iter  142\n",
      "The delta is  0.0005097102854598745\n",
      "This is iter  143\n",
      "The delta is  0.00048259569181396955\n",
      "This is iter  144\n",
      "The delta is  0.0004569234884996831\n",
      "This is iter  145\n",
      "The delta is  0.00043261694600005285\n",
      "This is iter  146\n",
      "The delta is  0.0004096034164895457\n",
      "This is iter  147\n",
      "The delta is  0.00038781411673838306\n",
      "This is iter  148\n",
      "The delta is  0.0003671839225205531\n",
      "This is iter  149\n",
      "The delta is  0.0003476511739428645\n",
      "This is iter  150\n",
      "The delta is  0.00032915749119766247\n",
      "This is iter  151\n",
      "The delta is  0.000311647600039322\n",
      "This is iter  152\n",
      "The delta is  0.0002950691666079308\n",
      "This is iter  153\n",
      "The delta is  0.00027937264099975323\n",
      "This is iter  154\n",
      "The delta is  0.0002645111091617025\n",
      "This is iter  155\n",
      "The delta is  0.0002504401526870481\n",
      "This is iter  156\n",
      "The delta is  0.00023711771606471643\n",
      "This is iter  157\n",
      "The delta is  0.00022450398096651725\n",
      "This is iter  158\n",
      "The delta is  0.00021256124724544634\n",
      "This is iter  159\n",
      "The delta is  0.0002012538202436076\n",
      "This is iter  160\n",
      "The delta is  0.000190547904132643\n",
      "This is iter  161\n",
      "The delta is  0.00018041150089231905\n",
      "This is iter  162\n",
      "The delta is  0.00017081431465726382\n",
      "This is iter  163\n",
      "The delta is  0.00016172766119382231\n",
      "This is iter  164\n",
      "The delta is  0.00015312438215175916\n",
      "This is iter  165\n",
      "The delta is  0.00014497876391317277\n",
      "This is iter  166\n",
      "The delta is  0.00013726646069756043\n",
      "This is iter  167\n",
      "The delta is  0.00012996442185908563\n",
      "This is iter  168\n",
      "The delta is  0.0001230508229213001\n",
      "This is iter  169\n",
      "The delta is  0.00011650500040971679\n",
      "This is iter  170\n",
      "The delta is  0.00011030739005590817\n",
      "This is iter  171\n",
      "The delta is  0.0001044394683269445\n",
      "This is iter  172\n",
      "The delta is  9.888369708832556e-05\n",
      "This is iter  173\n",
      "The delta is  9.362347115171588e-05\n",
      "This is iter  174\n",
      "The delta is  8.864306865774552e-05\n",
      "This is iter  175\n",
      "The delta is  8.39276040984771e-05\n",
      "This is iter  176\n",
      "The delta is  7.946298381256156e-05\n",
      "This is iter  177\n",
      "The delta is  7.523586386426473e-05\n",
      "This is iter  178\n",
      "The delta is  7.123361016425633e-05\n",
      "This is iter  179\n",
      "The delta is  6.744426070426357e-05\n",
      "This is iter  180\n",
      "The delta is  6.385648982032421e-05\n",
      "This is iter  181\n",
      "The delta is  6.045957431055626e-05\n",
      "This is iter  182\n",
      "The delta is  5.724336142165498e-05\n",
      "This is iter  183\n",
      "The delta is  5.419823848029637e-05\n",
      "This is iter  184\n",
      "The delta is  5.131510416944707e-05\n",
      "This is iter  185\n",
      "The delta is  4.8585341328788445e-05\n",
      "This is iter  186\n",
      "The delta is  4.6000791190436985e-05\n",
      "This is iter  187\n",
      "The delta is  4.3553729014433884e-05\n",
      "This is iter  188\n",
      "The delta is  4.123684097478986e-05\n",
      "This is iter  189\n",
      "The delta is  3.904320231740144e-05\n",
      "This is iter  190\n",
      "The delta is  3.696625665483566e-05\n",
      "This is iter  191\n",
      "The delta is  3.4999796383772264e-05\n",
      "This is iter  192\n",
      "The delta is  3.3137944104311146e-05\n",
      "This is iter  193\n",
      "The delta is  3.137513508377765e-05\n",
      "This is iter  194\n",
      "The delta is  2.970610061225898e-05\n",
      "This is iter  195\n",
      "The delta is  2.81258522392136e-05\n",
      "This is iter  196\n",
      "The delta is  2.6629666905364502e-05\n",
      "This is iter  197\n",
      "The delta is  2.5213072778029755e-05\n",
      "This is iter  198\n",
      "The delta is  2.3871835917788076e-05\n",
      "This is iter  199\n",
      "The delta is  2.2601947613054563e-05\n",
      "This is iter  200\n",
      "The delta is  2.139961240033017e-05\n",
      "This is iter  201\n",
      "The delta is  2.026123671683422e-05\n",
      "This is iter  202\n",
      "The delta is  1.918341816420366e-05\n",
      "This is iter  203\n",
      "The delta is  1.8162935347731946e-05\n",
      "This is iter  204\n",
      "The delta is  1.7196738227198693e-05\n",
      "This is iter  205\n",
      "The delta is  1.6281939011264512e-05\n",
      "This is iter  206\n",
      "The delta is  1.5415803538587625e-05\n",
      "This is iter  207\n",
      "The delta is  1.4595743081713408e-05\n",
      "This is iter  208\n",
      "The delta is  1.3819306634133e-05\n",
      "This is iter  209\n",
      "The delta is  1.3084173570376834e-05\n",
      "This is iter  210\n",
      "The delta is  1.2388146711117543e-05\n",
      "This is iter  211\n",
      "The delta is  1.1729145757755077e-05\n",
      "This is iter  212\n",
      "The delta is  1.110520107516777e-05\n",
      "This is iter  213\n",
      "The delta is  1.0514447815523909e-05\n",
      "This is iter  214\n",
      "The delta is  9.955120312099552e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0.        , -13.99989315, -19.99984167, -21.99982282,\n",
       "       -13.99989315, -17.99986052, -19.99984273, -19.99984167,\n",
       "       -19.99984167, -19.99984273, -17.99986052, -13.99989315,\n",
       "       -21.99982282, -19.99984167, -13.99989315,   0.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run your code, does it make sense?\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "V = policy_eval_v(random_policy, env)\n",
    "assert V.shape == (env.nS)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_gridworld_value(V):\n",
    "    plt.figure()\n",
    "    c = plt.pcolormesh(V, cmap='gray')\n",
    "    plt.colorbar(c)\n",
    "    plt.gca().invert_yaxis()  # In the array, first row = 0 is on top\n",
    "\n",
    "# Making a plot always helps\n",
    "plot_gridworld_value(V.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Policy Iteration (2 points)\n",
    "Using the policy evaluation algorithm we can implement policy iteration to find a good policy for this problem. Note that we do not need to use a discount_factor for episodic tasks but make sure your implementation can handle this correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dp_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dp_autograde.py\n",
    "\n",
    "def policy_iter_v(env, policy_eval_v=policy_eval_v, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Iteration Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_v: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6ff5cdc5bbcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's see what it does\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_iter_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_eval_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Policy Probability Distribution:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-56a6a31ae742>\u001b[0m in \u001b[0;36mpolicy_iter_v\u001b[0;34m(env, policy_eval_v, discount_factor)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's see what it does\n",
    "policy, v = policy_iter_v(env, policy_eval_v)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "def print_grid_policy(policy, symbols=[\"^\", \">\", \"v\", \"<\"]):\n",
    "    symbols = np.array(symbols)\n",
    "    for row in policy:\n",
    "        print(\"\".join(symbols[row]))\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n",
    "plot_gridworld_value(v.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Q-value Iteration (3 points)\n",
    "In this exercise you will implement the value iteration algorithm. However, because this algorithm is quite similar to the ones you implemented previously, we will spice things up a bit and use Q-values instead. Thus instead of using Bellman optimality equations for V you will use Bellman equations for Q. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%execwritefile -a dp_autograde.py\n",
    "\n",
    "def value_iter_q(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Q-value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all state-action pairs.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, Q) of the optimal policy and the optimal Q-value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with an all 0 Q-value function\n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what it does\n",
    "policy, Q = value_iter_q(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Q Function:\")\n",
    "print(Q)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see, the visualization of the Q function is quite clumsy and is not that easy to check \n",
    "# that all values make sense. However, you can easily create a V function from Q and policy to double\n",
    "# check that the values are what you would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dp_autograde.py file into codegrade.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
